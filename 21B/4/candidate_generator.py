#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å€™é€‰ç‚¹ç”Ÿæˆæ¨¡å— - GPRå®éªŒè®¾è®¡ç³»ç»Ÿ
åŠŸèƒ½ï¼šä½¿ç”¨æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·ç”Ÿæˆå€™é€‰å®éªŒç‚¹
"""

import numpy as np
import pandas as pd
# from pyDOE import lhs  # ä½¿ç”¨è‡ªå®šä¹‰LHSå®ç°
from sklearn.preprocessing import StandardScaler
from typing import Dict, List, Tuple
import itertools
import warnings
warnings.filterwarnings('ignore')

class CandidateGenerator:
    """å€™é€‰å®éªŒç‚¹ç”Ÿæˆå™¨"""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
        np.random.seed(random_state)
    
    def _lhs_sample(self, n_dims: int, n_samples: int) -> np.ndarray:
        """
        è‡ªå®šä¹‰æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·å®ç°
        
        Args:
            n_dims: ç»´åº¦æ•°
            n_samples: æ ·æœ¬æ•°
            
        Returns:
            samples: LHSæ ·æœ¬çŸ©é˜µ [0,1]^(n_samples x n_dims)
        """
        # åˆå§‹åŒ–æ ·æœ¬çŸ©é˜µ
        samples = np.zeros((n_samples, n_dims))
        
        # å¯¹æ¯ä¸ªç»´åº¦è¿›è¡ŒLHSé‡‡æ ·
        for dim in range(n_dims):
            # ç”Ÿæˆç­‰é—´è·çš„åŒºé—´
            intervals = np.linspace(0, 1, n_samples + 1)
            
            # åœ¨æ¯ä¸ªåŒºé—´å†…éšæœºé‡‡æ ·
            for i in range(n_samples):
                samples[i, dim] = np.random.uniform(intervals[i], intervals[i + 1])
            
            # éšæœºæ‰“ä¹±é¡ºåº
            np.random.shuffle(samples[:, dim])
        
        return samples
        
    def generate_candidates(self, variable_bounds: Dict, n_candidates: int = 1000,
                          scaler: StandardScaler = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç”Ÿæˆå€™é€‰å®éªŒç‚¹
        
        Args:
            variable_bounds: å˜é‡è¾¹ç•Œå­—å…¸
            n_candidates: å€™é€‰ç‚¹æ•°é‡
            scaler: æ•°æ®æ ‡å‡†åŒ–å™¨
            
        Returns:
            candidates_scaled: æ ‡å‡†åŒ–åçš„å€™é€‰ç‚¹
            candidates_original: åŸå§‹å°ºåº¦çš„å€™é€‰ç‚¹
        """
        print(f"ğŸ”„ ç”Ÿæˆ{n_candidates}ä¸ªå€™é€‰å®éªŒç‚¹...")
        
        # 1. åˆ†ç¦»è¿ç»­å˜é‡å’Œç¦»æ•£å˜é‡
        continuous_vars, discrete_vars = self._separate_variables(variable_bounds)
        
        # 2. ç”Ÿæˆè¿ç»­å˜é‡çš„LHSé‡‡æ ·
        print("  ğŸ“Š è¿ç»­å˜é‡LHSé‡‡æ ·...")
        continuous_samples = self._generate_continuous_samples(continuous_vars, n_candidates)
        
        # 3. ç”Ÿæˆç¦»æ•£å˜é‡çš„éšæœºé‡‡æ ·
        print("  ğŸ² ç¦»æ•£å˜é‡éšæœºé‡‡æ ·...")
        discrete_samples = self._generate_discrete_samples(discrete_vars, n_candidates)
        
        # 4. åˆå¹¶å€™é€‰ç‚¹
        print("  ğŸ”— åˆå¹¶å€™é€‰ç‚¹...")
        candidates_original = self._combine_samples(continuous_samples, discrete_samples, 
                                                   continuous_vars, discrete_vars)
        
        # 5. æ ‡å‡†åŒ–å€™é€‰ç‚¹
        if scaler is not None:
            print("  ğŸ“ æ ‡å‡†åŒ–å€™é€‰ç‚¹...")
            candidates_scaled = scaler.transform(candidates_original)
        else:
            candidates_scaled = candidates_original.copy()
        
        print(f"âœ… å€™é€‰ç‚¹ç”Ÿæˆå®Œæˆï¼")
        print(f"   ğŸ“ˆ å€™é€‰ç‚¹æ•°é‡: {len(candidates_original)}")
        print(f"   ğŸ“‹ ç‰¹å¾ç»´åº¦: {candidates_original.shape[1]}")
        
        return candidates_scaled, candidates_original
    
    def _separate_variables(self, variable_bounds: Dict) -> Tuple[Dict, Dict]:
        """åˆ†ç¦»è¿ç»­å˜é‡å’Œç¦»æ•£å˜é‡"""
        continuous_vars = {}
        discrete_vars = {}
        
        for var_name, bounds in variable_bounds.items():
            if isinstance(bounds, tuple):
                # è¿ç»­å˜é‡
                continuous_vars[var_name] = bounds
            elif isinstance(bounds, list):
                # ç¦»æ•£å˜é‡
                discrete_vars[var_name] = bounds
            else:
                raise ValueError(f"å˜é‡{var_name}çš„è¾¹ç•Œæ ¼å¼ä¸æ­£ç¡®")
        
        return continuous_vars, discrete_vars
    
    def _generate_continuous_samples(self, continuous_vars: Dict, n_samples: int) -> np.ndarray:
        """ç”Ÿæˆè¿ç»­å˜é‡çš„LHSé‡‡æ ·"""
        if not continuous_vars:
            return np.empty((n_samples, 0))
        
        n_continuous = len(continuous_vars)
        
        # LHSé‡‡æ · [0,1]^n
        lhs_samples = self._lhs_sample(n_continuous, n_samples)
        
        # ç¼©æ”¾åˆ°å®é™…èŒƒå›´
        continuous_samples = np.zeros_like(lhs_samples)
        for i, (var_name, (min_val, max_val)) in enumerate(continuous_vars.items()):
            continuous_samples[:, i] = min_val + lhs_samples[:, i] * (max_val - min_val)
        
        return continuous_samples
    
    def _generate_discrete_samples(self, discrete_vars: Dict, n_samples: int) -> np.ndarray:
        """ç”Ÿæˆç¦»æ•£å˜é‡çš„éšæœºé‡‡æ ·"""
        if not discrete_vars:
            return np.empty((n_samples, 0))
        
        n_discrete = len(discrete_vars)
        discrete_samples = np.zeros((n_samples, n_discrete))
        
        for i, (var_name, values) in enumerate(discrete_vars.items()):
            # éšæœºé€‰æ‹©ç¦»æ•£å€¼
            discrete_samples[:, i] = np.random.choice(values, size=n_samples)
        
        return discrete_samples
    
    def _combine_samples(self, continuous_samples: np.ndarray, discrete_samples: np.ndarray,
                        continuous_vars: Dict, discrete_vars: Dict) -> np.ndarray:
        """åˆå¹¶è¿ç»­å’Œç¦»æ•£æ ·æœ¬"""
        # ç¡®å®šå˜é‡é¡ºåºï¼ˆæŒ‰ç…§æ ‡å‡†é¡ºåºï¼‰
        var_order = ['T', 'total_mass', 'loading_ratio', 'Co_loading', 'ethanol_conc', 'loading_method']
        
        n_samples = max(continuous_samples.shape[0], discrete_samples.shape[0])
        combined_samples = np.zeros((n_samples, len(var_order)))
        
        continuous_idx = 0
        discrete_idx = 0
        
        for i, var_name in enumerate(var_order):
            if var_name in continuous_vars:
                combined_samples[:, i] = continuous_samples[:, continuous_idx]
                continuous_idx += 1
            elif var_name in discrete_vars:
                combined_samples[:, i] = discrete_samples[:, discrete_idx]
                discrete_idx += 1
            else:
                raise ValueError(f"æœªçŸ¥å˜é‡: {var_name}")
        
        return combined_samples
    
    def generate_grid_candidates(self, variable_bounds: Dict, 
                               scaler: StandardScaler = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç”Ÿæˆç½‘æ ¼å€™é€‰ç‚¹ï¼ˆç”¨äºå°è§„æ¨¡ç²¾ç¡®æœç´¢ï¼‰
        
        Args:
            variable_bounds: å˜é‡è¾¹ç•Œå­—å…¸
            scaler: æ•°æ®æ ‡å‡†åŒ–å™¨
            
        Returns:
            candidates_scaled: æ ‡å‡†åŒ–åçš„å€™é€‰ç‚¹
            candidates_original: åŸå§‹å°ºåº¦çš„å€™é€‰ç‚¹
        """
        print("ğŸ”„ ç”Ÿæˆç½‘æ ¼å€™é€‰ç‚¹...")
        
        # å®šä¹‰ç½‘æ ¼ç‚¹
        grid_points = {}
        
        for var_name, bounds in variable_bounds.items():
            if isinstance(bounds, tuple):
                # è¿ç»­å˜é‡ï¼šç”Ÿæˆç­‰é—´è·ç½‘æ ¼
                min_val, max_val = bounds
                if var_name == 'T':
                    # æ¸©åº¦ï¼šä½¿ç”¨å®é™…å®éªŒæ¸©åº¦ç‚¹
                    grid_points[var_name] = [250, 275, 300, 325, 350, 400, 450]
                elif var_name == 'total_mass':
                    # æ€»è´¨é‡ï¼š10ä¸ªç­‰é—´è·ç‚¹
                    grid_points[var_name] = np.linspace(min_val, max_val, 10).tolist()
                elif var_name == 'loading_ratio':
                    # è£…æ–™æ¯”ï¼š8ä¸ªç­‰é—´è·ç‚¹
                    grid_points[var_name] = np.linspace(min_val, max_val, 8).tolist()
            elif isinstance(bounds, list):
                # ç¦»æ•£å˜é‡ï¼šä½¿ç”¨æ‰€æœ‰å¯èƒ½å€¼
                grid_points[var_name] = bounds
        
        # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
        var_order = ['T', 'total_mass', 'loading_ratio', 'Co_loading', 'ethanol_conc', 'loading_method']
        grid_values = [grid_points[var] for var in var_order]
        
        # ç¬›å¡å°”ç§¯
        all_combinations = list(itertools.product(*grid_values))
        candidates_original = np.array(all_combinations)
        
        # æ ‡å‡†åŒ–
        if scaler is not None:
            candidates_scaled = scaler.transform(candidates_original)
        else:
            candidates_scaled = candidates_original.copy()
        
        print(f"âœ… ç½‘æ ¼å€™é€‰ç‚¹ç”Ÿæˆå®Œæˆï¼")
        print(f"   ğŸ“ˆ å€™é€‰ç‚¹æ•°é‡: {len(candidates_original)}")
        
        return candidates_scaled, candidates_original
    
    def filter_feasible_candidates(self, candidates_original: np.ndarray) -> np.ndarray:
        """
        è¿‡æ»¤å¯è¡Œçš„å€™é€‰ç‚¹
        
        Args:
            candidates_original: åŸå§‹å€™é€‰ç‚¹
            
        Returns:
            feasible_mask: å¯è¡Œç‚¹çš„å¸ƒå°”æ©ç 
        """
        n_candidates = len(candidates_original)
        feasible_mask = np.ones(n_candidates, dtype=bool)
        
        # å˜é‡ç´¢å¼•
        var_indices = {
            'T': 0, 'total_mass': 1, 'loading_ratio': 2,
            'Co_loading': 3, 'ethanol_conc': 4, 'loading_method': 5
        }
        
        # çº¦æŸæ¡ä»¶
        for i in range(n_candidates):
            candidate = candidates_original[i]
            
            # 1. æ¸©åº¦çº¦æŸ
            T = candidate[var_indices['T']]
            if T < 250 or T > 450:
                feasible_mask[i] = False
                continue
            
            # 2. è´¨é‡çº¦æŸ
            total_mass = candidate[var_indices['total_mass']]
            if total_mass < 20 or total_mass > 400:
                feasible_mask[i] = False
                continue
            
            # 3. è£…æ–™æ¯”çº¦æŸ
            loading_ratio = candidate[var_indices['loading_ratio']]
            if loading_ratio < 0.33 or loading_ratio > 2.03:
                feasible_mask[i] = False
                continue
            
            # 4. å·¥è‰ºçº¦æŸï¼šé«˜æ¸©+é«˜Coè´Ÿè½½é‡å¯èƒ½ä¸ç¨³å®š
            Co_loading = candidate[var_indices['Co_loading']]
            if T > 400 and Co_loading >= 5.0:
                feasible_mask[i] = False
                continue
            
            # 5. è£…æ–™çº¦æŸï¼šæç«¯è£…æ–™æ¯”+é«˜æ¸©å¯èƒ½æœ‰é—®é¢˜
            if T > 350 and (loading_ratio < 0.4 or loading_ratio > 1.8):
                feasible_mask[i] = False
                continue
        
        print(f"ğŸ” å¯è¡Œæ€§è¿‡æ»¤å®Œæˆ: {feasible_mask.sum()}/{len(feasible_mask)} ç‚¹å¯è¡Œ")
        
        return feasible_mask
    
    def save_candidates(self, candidates_original: np.ndarray, filepath: str):
        """ä¿å­˜å€™é€‰ç‚¹"""
        feature_names = ['T', 'total_mass', 'loading_ratio', 'Co_loading', 'ethanol_conc', 'loading_method']
        
        df = pd.DataFrame(candidates_original, columns=feature_names)
        df.to_csv(filepath, index=False)
        print(f"ğŸ’¾ å€™é€‰ç‚¹å·²ä¿å­˜: {filepath}")

def main():
    """æµ‹è¯•å€™é€‰ç‚¹ç”Ÿæˆæ¨¡å—"""
    from data_processor import DataProcessor
    
    print("ğŸ§ª æµ‹è¯•å€™é€‰ç‚¹ç”Ÿæˆæ¨¡å—...")
    
    try:
        # 1. è·å–å˜é‡è¾¹ç•Œ
        processor = DataProcessor()
        variable_bounds = processor.get_variable_bounds()
        
        print("ğŸ“ å˜é‡è¾¹ç•Œ:")
        for var, bounds in variable_bounds.items():
            print(f"  {var}: {bounds}")
        
        # 2. ç”Ÿæˆå€™é€‰ç‚¹
        generator = CandidateGenerator(random_state=42)
        
        # LHSé‡‡æ ·
        candidates_scaled, candidates_original = generator.generate_candidates(
            variable_bounds, n_candidates=1000
        )
        
        # ç½‘æ ¼é‡‡æ ·
        grid_scaled, grid_original = generator.generate_grid_candidates(variable_bounds)
        
        # 3. å¯è¡Œæ€§è¿‡æ»¤
        feasible_mask = generator.filter_feasible_candidates(candidates_original)
        feasible_candidates = candidates_original[feasible_mask]
        
        print(f"\nğŸ“Š å€™é€‰ç‚¹ç»Ÿè®¡:")
        print(f"LHSå€™é€‰ç‚¹: {len(candidates_original)}")
        print(f"ç½‘æ ¼å€™é€‰ç‚¹: {len(grid_original)}")
        print(f"å¯è¡ŒLHSå€™é€‰ç‚¹: {len(feasible_candidates)}")
        
        # 4. ä¿å­˜å€™é€‰ç‚¹
        generator.save_candidates(feasible_candidates, 'feasible_candidates.csv')
        generator.save_candidates(grid_original, 'grid_candidates.csv')
        
        print("\nâœ… å€™é€‰ç‚¹ç”Ÿæˆæ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()