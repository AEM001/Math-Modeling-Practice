#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPRæ¨¡å‹æ„å»ºæ¨¡å— - GPRå®éªŒè®¾è®¡ç³»ç»Ÿ
åŠŸèƒ½ï¼šé«˜æ–¯è¿‡ç¨‹å›å½’æ¨¡å‹æ„å»ºã€è®­ç»ƒã€éªŒè¯
"""

import numpy as np
import pandas as pd
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel
from sklearn.model_selection import cross_val_score, KFold, LeaveOneOut
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Tuple, Dict, Optional
import warnings
warnings.filterwarnings('ignore')

# æ›´ç¨³å¥çš„ä¸­æ–‡å­—ä½“è®¾ç½®
import platform, os
from matplotlib import font_manager as fm

def _set_chinese_font():
    system = platform.system()
    if system == 'Darwin':  # macOS å¸¸è§ä¸­æ–‡å­—ä½“
        candidates = ['PingFang SC', 'Hiragino Sans GB', 'Heiti SC', 'Songti SC', 'STHeiti', 'STSong', 'Arial Unicode MS']
        font_dirs = [
            '/System/Library/Fonts',
            '/Library/Fonts',
            os.path.expanduser('~/Library/Fonts')
        ]
        file_keywords = ['PingFang', 'Hiragino', 'Heiti', 'Songti', 'STHeiti', 'STSong']
    elif system == 'Windows':
        candidates = ['Microsoft YaHei', 'SimHei', 'SimSun', 'NSimSun']
        font_dirs = [os.path.join(os.environ.get('WINDIR', 'C:\\Windows'), 'Fonts')]
        file_keywords = ['yahei', 'simhei', 'simsun']
    else:  # Linux
        candidates = ['Noto Sans CJK SC', 'WenQuanYi Micro Hei', 'Source Han Sans SC', 'Droid Sans Fallback', 'DejaVu Sans']
        font_dirs = ['/usr/share/fonts', '/usr/local/share/fonts', os.path.expanduser('~/.local/share/fonts')]
        file_keywords = ['NotoSansCJK', 'WenQuanYi', 'SourceHanSans', 'DroidSansFallback']

    # 1) ä¼˜å…ˆé€šè¿‡ family åç§°ä¸¥æ ¼æŸ¥æ‰¾
    for name in candidates:
        try:
            path = fm.findfont(fm.FontProperties(family=name), fallback_to_default=False)
            if path and os.path.exists(path):
                plt.rcParams['font.family'] = 'sans-serif'
                plt.rcParams['font.sans-serif'] = [name]
                plt.rcParams['axes.unicode_minus'] = False
                print(f'ä½¿ç”¨ä¸­æ–‡å­—ä½“: {name} -> {path}')
                return name
        except Exception:
            continue

    # 2) æ‰«æç³»ç»Ÿå­—ä½“ç›®å½•ï¼Œå°è¯•åŠ¨æ€æ³¨å†Œï¼ˆå« .ttf/.otf/.ttcï¼‰
    try:
        for d in font_dirs:
            if not os.path.isdir(d):
                continue
            for fname in os.listdir(d):
                lower = fname.lower()
                if any(k.lower() in lower for k in file_keywords) and (lower.endswith('.ttf') or lower.endswith('.otf') or lower.endswith('.ttc')):
                    fpath = os.path.join(d, fname)
                    try:
                        fm.fontManager.addfont(fpath)
                    except Exception:
                        # æŸäº› .ttc å¯èƒ½æ— æ³•ç›´æ¥ addfontï¼Œå¿½ç•¥é”™è¯¯ç»§ç»­
                        pass
        fm._rebuild()  # åˆ·æ–°å­—ä½“ç¼“å­˜
        installed = {f.name for f in fm.fontManager.ttflist}
        for name in candidates:
            if name in installed:
                plt.rcParams['font.family'] = 'sans-serif'
                plt.rcParams['font.sans-serif'] = [name]
                plt.rcParams['axes.unicode_minus'] = False
                print(f'ä½¿ç”¨ä¸­æ–‡å­—ä½“(åŠ¨æ€æ³¨å†Œ): {name}')
                return name
    except Exception:
        pass

    # 3) å†æ¬¡å°è¯•å®½æ¾åŒ¹é…ä»»ä¸€å·²å®‰è£… CJK å­—ä½“
    installed_fonts = [(f.name, getattr(f, 'fname', '')) for f in fm.fontManager.ttflist]
    for fam, fpath in installed_fonts:
        if any(k.lower() in fam.lower() for k in ['pingfang', 'hiragino', 'heiti', 'song', 'noto', 'source han', 'wqy', 'cjk', 'æ±‰', 'é»‘ä½“', 'å®‹ä½“']):
            plt.rcParams['font.family'] = 'sans-serif'
            plt.rcParams['font.sans-serif'] = [fam]
            plt.rcParams['axes.unicode_minus'] = False
            print(f'ä½¿ç”¨ä¸­æ–‡å­—ä½“(å®½æ¾åŒ¹é…): {fam} -> {fpath}')
            return fam

    # 4) æœ€åå…œåº•
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    print('æœªæ‰¾åˆ°åˆé€‚ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨ DejaVu Sans å…œåº•ï¼ˆå¯èƒ½æ— æ³•æ˜¾ç¤ºä¸­æ–‡ï¼‰')
    return 'DejaVu Sans'

# æ¨¡å—å¯¼å…¥æ—¶å…ˆè®¾ç½®ä¸€æ¬¡
_set_chinese_font()

class GPRModel:
    """é«˜æ–¯è¿‡ç¨‹å›å½’æ¨¡å‹ç±»"""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
        self.model = None
        self.is_fitted = False
        self.validation_results = {}
        
    def build_and_train(self, X: np.ndarray, y: np.ndarray, 
                       optimize_hyperparams: bool = True) -> Dict:
        """
        æ„å»ºå¹¶è®­ç»ƒGPRæ¨¡å‹
        
        Args:
            X: æ ‡å‡†åŒ–åçš„ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡
            optimize_hyperparams: æ˜¯å¦ä¼˜åŒ–è¶…å‚æ•°
            
        Returns:
            training_results: è®­ç»ƒç»“æœå­—å…¸
        """
        print(" å¼€å§‹æ„å»ºGPRæ¨¡å‹...")
        
        # 1. å®šä¹‰æ ¸å‡½æ•°
        print("   é…ç½®æ ¸å‡½æ•°...")
        kernel = self._build_kernel(X.shape[1])
        
        # 2. åˆ›å»ºGPRæ¨¡å‹
        print("   åˆ›å»ºGPRæ¨¡å‹...")
        self.model = GaussianProcessRegressor(
            kernel=kernel,
            alpha=0.5,  # ä¿®æ­£ï¼šå¢åŠ æ­£åˆ™åŒ–é¡¹ï¼Œå¯¹æŠ—è¿‡æ‹Ÿåˆ
            normalize_y=True,  # æ ‡å‡†åŒ–ç›®æ ‡å˜é‡
            n_restarts_optimizer=10 if optimize_hyperparams else 0,
            random_state=self.random_state
        )
        
        # 3. è®­ç»ƒæ¨¡å‹
        print("   è®­ç»ƒæ¨¡å‹...")
        self.model.fit(X, y)
        self.is_fitted = True
        
        # 4. æ¨¡å‹éªŒè¯
        print("   æ¨¡å‹éªŒè¯...")
        validation_results = self._validate_model(X, y)
        
        # 5. æ•´ç†è®­ç»ƒç»“æœ
        training_results = {
            'kernel_params': self._extract_kernel_params(),
            'log_marginal_likelihood': self.model.log_marginal_likelihood_value_,
            'validation_results': validation_results,
            'model_summary': self._get_model_summary(X, y)
        }
        
        print(f" GPRæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"    äº¤å‰éªŒè¯RÂ²: {validation_results['cv_r2_mean']:.4f} (Â±{validation_results['cv_r2_std']:.4f})")
        print(f"    è®­ç»ƒé›†RÂ²: {validation_results['train_r2']:.4f}")
        print(f"    å¯¹æ•°è¾¹é™…ä¼¼ç„¶: {self.model.log_marginal_likelihood_value_:.4f}")
        
        return training_results
    
    def _build_kernel(self, n_features: int):
        """æ„å»ºRBFæ ¸å‡½æ•°"""
        # ä¸ºæ¯ä¸ªç‰¹å¾è®¾ç½®ç‹¬ç«‹çš„é•¿åº¦å°ºåº¦
        length_scale = np.ones(n_features)
        length_scale_bounds = (1e-2, 1e2)
        
        # RBFæ ¸ + å¸¸æ•°æ ¸ + ç™½å™ªå£°æ ¸
        kernel = (C(1.0, (1e-3, 1e3)) * 
                 RBF(length_scale=length_scale, length_scale_bounds=length_scale_bounds) + 
                 WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e-1)))
        
        return kernel
    
    def _validate_model(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """æ¨¡å‹éªŒè¯"""
        results = {}
        
        # è®­ç»ƒé›†æ€§èƒ½
        y_pred_train = self.model.predict(X)
        results['train_r2'] = r2_score(y, y_pred_train)
        results['train_rmse'] = np.sqrt(mean_squared_error(y, y_pred_train))
        results['train_mae'] = mean_absolute_error(y, y_pred_train)
        
        # ç•™ä¸€äº¤å‰éªŒè¯
        loo = LeaveOneOut()
        try:
            cv_scores = cross_val_score(self.model, X, y, cv=loo, scoring='r2')
            # è¿‡æ»¤æ‰nanå€¼
            cv_scores_clean = cv_scores[~np.isnan(cv_scores)]
            if len(cv_scores_clean) > 0:
                results['cv_r2_mean'] = cv_scores_clean.mean()
                results['cv_r2_std'] = cv_scores_clean.std()
                results['cv_scores'] = cv_scores_clean.tolist()
            else:
                # å¦‚æœæ‰€æœ‰å€¼éƒ½æ˜¯nanï¼Œå›é€€åˆ°5æŠ˜äº¤å‰éªŒè¯
                cv_scores = cross_val_score(self.model, X, y, cv=5, scoring='r2')
                results['cv_r2_mean'] = cv_scores.mean()
                results['cv_r2_std'] = cv_scores.std()
                results['cv_scores'] = cv_scores.tolist()
                print("   ç•™ä¸€äº¤å‰éªŒè¯å‡ºç°æ•°å€¼é—®é¢˜ï¼Œå›é€€åˆ°5æŠ˜äº¤å‰éªŒè¯")
        except:
            # å¦‚æœç•™ä¸€äº¤å‰éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯
            cv_scores = cross_val_score(self.model, X, y, cv=5, scoring='r2')
            results['cv_r2_mean'] = cv_scores.mean()
            results['cv_r2_std'] = cv_scores.std()
            results['cv_scores'] = cv_scores.tolist()
            print("   ç•™ä¸€äº¤å‰éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯")
        
        # RMSEäº¤å‰éªŒè¯
        try:
            cv_rmse_scores = cross_val_score(self.model, X, y, cv=loo, 
                                            scoring='neg_mean_squared_error')
            cv_rmse_scores_clean = cv_rmse_scores[~np.isnan(cv_rmse_scores)]
            if len(cv_rmse_scores_clean) > 0:
                results['cv_rmse_mean'] = np.sqrt(-cv_rmse_scores_clean.mean())
                results['cv_rmse_std'] = np.sqrt(cv_rmse_scores_clean.std())
            else:
                cv_rmse_scores = cross_val_score(self.model, X, y, cv=5, 
                                                scoring='neg_mean_squared_error')
                results['cv_rmse_mean'] = np.sqrt(-cv_rmse_scores.mean())
                results['cv_rmse_std'] = np.sqrt(cv_rmse_scores.std())
        except:
            cv_rmse_scores = cross_val_score(self.model, X, y, cv=5, 
                                            scoring='neg_mean_squared_error')
            results['cv_rmse_mean'] = np.sqrt(-cv_rmse_scores.mean())
            results['cv_rmse_std'] = np.sqrt(cv_rmse_scores.std())
        
        # æ¨¡å‹è´¨é‡è¯„ä¼°
        results['quality_assessment'] = self._assess_model_quality(results)
        
        return results
    
    def _assess_model_quality(self, results: Dict) -> Dict:
        """è¯„ä¼°æ¨¡å‹è´¨é‡"""
        assessment = {}
        
        cv_r2 = results['cv_r2_mean']
        cv_r2_std = results['cv_r2_std']
        
        # RÂ²è¯„ä¼°
        if cv_r2 > 0.8:
            assessment['r2_level'] = 'excellent'
        elif cv_r2 > 0.7:
            assessment['r2_level'] = 'good'
        elif cv_r2 > 0.5:
            assessment['r2_level'] = 'acceptable'
        else:
            assessment['r2_level'] = 'poor'
        
        # ç¨³å®šæ€§è¯„ä¼°
        if cv_r2_std < 0.05:
            assessment['stability'] = 'high'
        elif cv_r2_std < 0.1:
            assessment['stability'] = 'medium'
        else:
            assessment['stability'] = 'low'
        
        # è¿‡æ‹Ÿåˆæ£€æŸ¥
        train_r2 = results['train_r2']
        overfitting_gap = train_r2 - cv_r2
        if overfitting_gap < 0.05:
            assessment['overfitting_risk'] = 'low'
        elif overfitting_gap < 0.15:
            assessment['overfitting_risk'] = 'medium'
        else:
            assessment['overfitting_risk'] = 'high'
        
        # æ€»ä½“è¯„ä¼°
        if (assessment['r2_level'] in ['excellent', 'good'] and 
            assessment['stability'] in ['high', 'medium'] and
            assessment['overfitting_risk'] in ['low', 'medium']):
            assessment['overall'] = 'suitable_for_optimization'
        else:
            assessment['overall'] = 'needs_improvement'
        
        return assessment
    
    def _extract_kernel_params(self) -> Dict:
        """æå–æ ¸å‡½æ•°å‚æ•°"""
        if not self.is_fitted:
            return {}
        
        kernel = self.model.kernel_
        params = {}
        
        # æå–å„ç»„ä»¶å‚æ•°
        if hasattr(kernel, 'k1') and hasattr(kernel, 'k2'):
            # å¤åˆæ ¸å‡½æ•°
            k1 = kernel.k1  # ConstantKernel * RBF
            k2 = kernel.k2  # WhiteKernel
            
            if hasattr(k1, 'k1') and hasattr(k1, 'k2'):
                # ConstantKernel
                params['constant_value'] = float(k1.k1.constant_value)
                # RBF
                params['length_scale'] = k1.k2.length_scale.tolist()
            
            # WhiteKernel
            if hasattr(k2, 'noise_level'):
                params['noise_level'] = float(k2.noise_level)
        
        return params
    
    def _get_model_summary(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """è·å–æ¨¡å‹æ‘˜è¦"""
        return {
            'n_samples': X.shape[0],
            'n_features': X.shape[1],
            'target_range': [float(y.min()), float(y.max())],
            'target_mean': float(y.mean()),
            'target_std': float(y.std())
        }
    
    def predict(self, X: np.ndarray, return_std: bool = True) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        é¢„æµ‹
        
        Args:
            X: è¾“å…¥ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–åï¼‰
            return_std: æ˜¯å¦è¿”å›é¢„æµ‹æ ‡å‡†å·®
            
        Returns:
            y_pred: é¢„æµ‹å‡å€¼
            y_std: é¢„æµ‹æ ‡å‡†å·®ï¼ˆå¦‚æœreturn_std=Trueï¼‰
        """
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨build_and_trainæ–¹æ³•")
        
        if return_std:
            y_pred, y_std = self.model.predict(X, return_std=True)
            return y_pred, y_std
        else:
            y_pred = self.model.predict(X, return_std=False)
            return y_pred, None
    
    def plot_validation_results(self, X: np.ndarray, y: np.ndarray, save_path: str = None):
        """ç»˜åˆ¶éªŒè¯ç»“æœ"""
        if not self.is_fitted:
            print(" æ¨¡å‹å°šæœªè®­ç»ƒï¼Œæ— æ³•ç»˜åˆ¶éªŒè¯ç»“æœ")
            return
        
        # é¢„æµ‹
        y_pred, y_std = self.predict(X, return_std=True)
        
        # åˆ›å»ºå›¾å½¢
        _set_chinese_font()
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('GPRæ¨¡å‹éªŒè¯ç»“æœ', fontsize=16, fontweight='bold')
        
        # 1. é¢„æµ‹vså®é™…
        axes[0, 0].scatter(y, y_pred, alpha=0.6, color='blue')
        axes[0, 0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('å®é™…å€¼')
        axes[0, 0].set_ylabel('é¢„æµ‹å€¼')
        axes[0, 0].set_title(f'é¢„æµ‹vså®é™… (RÂ²={r2_score(y, y_pred):.3f})')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. æ®‹å·®å›¾
        residuals = y - y_pred
        axes[0, 1].scatter(y_pred, residuals, alpha=0.6, color='green')
        axes[0, 1].axhline(y=0, color='r', linestyle='--')
        axes[0, 1].set_xlabel('é¢„æµ‹å€¼')
        axes[0, 1].set_ylabel('æ®‹å·®')
        axes[0, 1].set_title('æ®‹å·®åˆ†å¸ƒ')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. é¢„æµ‹ä¸ç¡®å®šæ€§
        sorted_indices = np.argsort(y_pred)
        axes[1, 0].fill_between(range(len(y)), 
                               (y_pred - 1.96*y_std)[sorted_indices],
                               (y_pred + 1.96*y_std)[sorted_indices],
                               alpha=0.3, color='gray', label='95%ç½®ä¿¡åŒºé—´')
        axes[1, 0].scatter(range(len(y)), y[sorted_indices], alpha=0.6, color='blue', label='å®é™…å€¼')
        axes[1, 0].plot(range(len(y)), y_pred[sorted_indices], color='red', label='é¢„æµ‹å€¼')
        axes[1, 0].set_xlabel('æ ·æœ¬ç´¢å¼•ï¼ˆæŒ‰é¢„æµ‹å€¼æ’åºï¼‰')
        axes[1, 0].set_ylabel('C4çƒ¯çƒƒæ”¶ç‡')
        axes[1, 0].set_title('é¢„æµ‹ä¸ç¡®å®šæ€§')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. äº¤å‰éªŒè¯ç»“æœ
        cv_scores = self.validation_results.get('cv_scores', [])
        if cv_scores:
            axes[1, 1].bar(range(1, len(cv_scores)+1), cv_scores, color='orange', alpha=0.7)
            axes[1, 1].axhline(y=np.mean(cv_scores), color='r', linestyle='--', 
                              label=f'å¹³å‡RÂ²={np.mean(cv_scores):.3f}')
            axes[1, 1].set_xlabel('äº¤å‰éªŒè¯æŠ˜æ•°')
            axes[1, 1].set_ylabel('RÂ²åˆ†æ•°')
            axes[1, 1].set_title('äº¤å‰éªŒè¯ç»“æœ')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f" éªŒè¯ç»“æœå›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def save_model_info(self, filepath: str, training_results: Dict):
        """ä¿å­˜æ¨¡å‹ä¿¡æ¯"""
        model_info = {
            'model_type': 'GaussianProcessRegressor',
            'kernel_type': 'RBF + Constant + WhiteKernel',
            'training_results': training_results,
            'is_fitted': self.is_fitted
        }
        
        # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
        info_df = pd.DataFrame([model_info])
        info_df.to_csv(filepath, index=False)
        print(f" æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {filepath}")

def main():
    """æµ‹è¯•GPRæ¨¡å‹æ¨¡å—"""
    from data_processor import DataProcessor
    
    print(" æµ‹è¯•GPRæ¨¡å‹æ¨¡å—...")
    
    # 1. åŠ è½½æ•°æ®
    processor = DataProcessor()
    try:
        X, y, data_info = processor.load_and_prepare_data('é™„ä»¶1.csv', 'æ¯ç»„æŒ‡æ ‡.csv')
        
        # 2. æ„å»ºå’Œè®­ç»ƒGPRæ¨¡å‹
        gpr = GPRModel(random_state=42)
        training_results = gpr.build_and_train(X, y)
        
        # 3. æµ‹è¯•é¢„æµ‹
        y_pred, y_std = gpr.predict(X[:5], return_std=True)
        print(f"\nğŸ”® é¢„æµ‹æµ‹è¯•:")
        print(f"å‰5ä¸ªæ ·æœ¬é¢„æµ‹å€¼: {y_pred}")
        print(f"é¢„æµ‹æ ‡å‡†å·®: {y_std}")
        
        # 4. ç»˜åˆ¶éªŒè¯ç»“æœ
        gpr.validation_results = training_results['validation_results']
        gpr.plot_validation_results(X, y, 'gpr_validation_results.png')
        
        # 5. ä¿å­˜æ¨¡å‹ä¿¡æ¯
        gpr.save_model_info('gpr_model_info.csv', training_results)
        
        print("\n GPRæ¨¡å‹æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f" æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()